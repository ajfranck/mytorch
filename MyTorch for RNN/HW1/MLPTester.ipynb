{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe5a7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "import mytorch\n",
    "from mytorch import nn as mynn\n",
    "from models import MLP0, MLP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c637116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up synthetic data\n",
    "N = 10\n",
    "num_inputs = 7\n",
    "num_outputs = 2\n",
    "\n",
    "# numpy/our versions\n",
    "W = np.random.rand(num_inputs, num_outputs)\n",
    "b = np.random.rand(num_outputs)\n",
    "X = np.random.randn(N, num_inputs)\n",
    "Y = X @ W + np.outer(np.ones(N), b) + 0.5 * np.random.randn(N, num_outputs)\n",
    "\n",
    "# converted torch versions\n",
    "Xt = torch.tensor(X).float()\n",
    "Wt = torch.tensor(W).float()\n",
    "bt = torch.tensor(b).float()\n",
    "Yt = torch.tensor(Y).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2dca35",
   "metadata": {},
   "source": [
    "# MLP0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228bf728",
   "metadata": {},
   "source": [
    "## Test `forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b7dfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTorch:\n",
      " [[ 3.86002782  2.99438936]\n",
      " [-0.79696783 -0.2794177 ]\n",
      " [-0.16700786  0.08508616]\n",
      " [-1.68222523 -0.6309244 ]\n",
      " [-0.82226997 -0.4766995 ]\n",
      " [ 3.64463101  1.33491015]\n",
      " [-2.59587695 -0.72561625]\n",
      " [-1.96711607 -1.04104274]\n",
      " [-1.24519469 -1.12006223]\n",
      " [ 0.83728668  1.70837926]] \n",
      "\n",
      "PyTorch:\n",
      " tensor([[ 3.8600,  2.9944],\n",
      "        [-0.7970, -0.2794],\n",
      "        [-0.1670,  0.0851],\n",
      "        [-1.6822, -0.6309],\n",
      "        [-0.8223, -0.4767],\n",
      "        [ 3.6446,  1.3349],\n",
      "        [-2.5959, -0.7256],\n",
      "        [-1.9671, -1.0410],\n",
      "        [-1.2452, -1.1201],\n",
      "        [ 0.8373,  1.7084]]) \n",
      "\n",
      "Difference: 3.7775427099892897e-07\n"
     ]
    }
   ],
   "source": [
    "# initialize model and fix weights to true values\n",
    "mlp0 = MLP0(num_inputs, num_outputs)\n",
    "mlp0.layers[0].W = W\n",
    "mlp0.layers[0].b = b\n",
    "\n",
    "# initialize torch model, loss, optimizer\n",
    "net = nn.Sequential(nn.Linear(num_inputs, num_outputs))\n",
    "net[0].weight = nn.Parameter(Wt.T)\n",
    "net[0].bias = nn.Parameter(bt)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1, momentum=0.0)\n",
    "\n",
    "my_out = mlp0.forward(X)\n",
    "torch_out = net(Xt)\n",
    "\n",
    "print('MyTorch:\\n', my_out, '\\n')\n",
    "print('PyTorch:\\n', torch_out.data, '\\n')\n",
    "print('Difference:', np.linalg.norm(my_out - torch_out.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700cdb6",
   "metadata": {},
   "source": [
    "## Test `backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f75882e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTorch dLdW:\n",
      " [[-0.21218814  0.05111207]\n",
      " [-0.08868613  0.02084339]\n",
      " [ 0.078828    0.17021322]\n",
      " [ 0.06374441 -0.16389866]\n",
      " [ 0.17499219 -0.0323467 ]\n",
      " [ 0.21739307 -0.07383894]\n",
      " [-0.08884962 -0.10607492]] \n",
      "\n",
      "PyTorch dLdW:\n",
      " tensor([[-0.2122,  0.0511],\n",
      "        [-0.0887,  0.0208],\n",
      "        [ 0.0788,  0.1702],\n",
      "        [ 0.0637, -0.1639],\n",
      "        [ 0.1750, -0.0323],\n",
      "        [ 0.2174, -0.0738],\n",
      "        [-0.0888, -0.1061]]) \n",
      "\n",
      "MyTorch dLdb:\n",
      " [-0.34788953  0.21548112] \n",
      "\n",
      "PyTorch dLdb:\n",
      " tensor([-0.3479,  0.2155]) \n",
      "\n",
      "Difference in dLdW: 1.5759888413168285e-07\n",
      "Difference in dLdb: 1.6025198587496917e-08\n"
     ]
    }
   ],
   "source": [
    "my_mse_fn = mynn.MSELoss()\n",
    "my_mse = my_mse_fn.forward(my_out, Y)\n",
    "dLdZ = my_mse_fn.backward()\n",
    "mlp0.backward(dLdZ)\n",
    "my_dLdW = mlp0.layers[0].dLdW\n",
    "my_dLdb = mlp0.layers[0].dLdb\n",
    "\n",
    "optimizer.zero_grad()\n",
    "torch_loss_fn = nn.MSELoss()\n",
    "torch_loss = torch_loss_fn(torch_out, Yt)\n",
    "torch_loss.backward(retain_graph=True)\n",
    "torch_dLdW = net[0].weight.grad.data\n",
    "torch_dLdb = net[0].bias.grad.data\n",
    "\n",
    "print('MyTorch dLdW:\\n', my_dLdW, '\\n')\n",
    "print('PyTorch dLdW:\\n', torch_dLdW.T, '\\n')\n",
    "print('MyTorch dLdb:\\n', my_dLdb, '\\n')\n",
    "print('PyTorch dLdb:\\n', torch_dLdb, '\\n')\n",
    "\n",
    "print('Difference in dLdW:', np.linalg.norm(my_dLdW.T - torch_dLdW.numpy()))\n",
    "print('Difference in dLdb:', np.linalg.norm(my_dLdb.flatten() - torch_dLdb.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9df06",
   "metadata": {},
   "source": [
    "## Test a single optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17aab683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTorch Wk:\n",
      " [[0.64096616 0.2778433 ]\n",
      " [0.61522442 0.40386958]\n",
      " [0.29952571 0.16312084]\n",
      " [0.88400252 0.59914513]\n",
      " [0.28828544 0.71377332]\n",
      " [0.58806773 0.79375926]\n",
      " [0.69368298 0.18085436]] \n",
      "\n",
      "PyTorch Wk:\n",
      " tensor([[0.6410, 0.2778],\n",
      "        [0.6152, 0.4039],\n",
      "        [0.2995, 0.1631],\n",
      "        [0.8840, 0.5991],\n",
      "        [0.2883, 0.7138],\n",
      "        [0.5881, 0.7938],\n",
      "        [0.6937, 0.1809]]) \n",
      "\n",
      "MyTorch bk:\n",
      " [[0.69026393 0.39344826]] \n",
      "\n",
      "PyTorch bk:\n",
      " tensor([0.6903, 0.3934])\n",
      "Difference in Wk: 1.8491735404934512e-07\n",
      "Difference in bk: 6.96111754757384e-08\n"
     ]
    }
   ],
   "source": [
    "# my SGD step\n",
    "my_optimizer = mytorch.optim.SGD(mlp0, lr=1)\n",
    "my_optimizer.step()\n",
    "my_Wk = mlp0.layers[0].W\n",
    "my_bk = mlp0.layers[0].b\n",
    "\n",
    "# torch SGD step\n",
    "optimizer.zero_grad()\n",
    "torch_loss.backward(retain_graph=True)\n",
    "optimizer.step()\n",
    "torch_Wk = net[0].weight.data\n",
    "torch_bk = net[0].bias.data\n",
    "\n",
    "print('MyTorch Wk:\\n', my_Wk, '\\n')\n",
    "print('PyTorch Wk:\\n', torch_Wk.T, '\\n')\n",
    "print('MyTorch bk:\\n', my_bk, '\\n')\n",
    "print('PyTorch bk:\\n', torch_bk)\n",
    "\n",
    "print('Difference in Wk:', np.linalg.norm(my_Wk.T - torch_Wk.numpy()))\n",
    "print('Difference in bk:', np.linalg.norm(my_bk.flatten() - torch_bk.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babef7d0",
   "metadata": {},
   "source": [
    "# MLP1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f72747c",
   "metadata": {},
   "source": [
    "## Test `forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42dd2714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTorch:\n",
      " [[-0.43635382  0.38668833]\n",
      " [-0.54950023  0.3628647 ]\n",
      " [-0.54950023  0.3628647 ]\n",
      " [-0.51382037  0.21551731]\n",
      " [-0.50974625  0.37123513]\n",
      " [-0.54950023  0.3628647 ]\n",
      " [-0.38876143  0.29533438]\n",
      " [-0.56818172  0.27561022]\n",
      " [-0.52001079  0.3601708 ]\n",
      " [-0.61013367  0.07966791]] \n",
      "\n",
      "PyTorch:\n",
      " tensor([[-0.4364,  0.3867],\n",
      "        [-0.5495,  0.3629],\n",
      "        [-0.5495,  0.3629],\n",
      "        [-0.5138,  0.2155],\n",
      "        [-0.5097,  0.3712],\n",
      "        [-0.5495,  0.3629],\n",
      "        [-0.3888,  0.2953],\n",
      "        [-0.5682,  0.2756],\n",
      "        [-0.5200,  0.3602],\n",
      "        [-0.6101,  0.0797]]) \n",
      "\n",
      "Difference: 7.751443391709816e-08\n"
     ]
    }
   ],
   "source": [
    "num_hiddens=3\n",
    "\n",
    "# initialize torch model, loss, optimizer\n",
    "net = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(num_hiddens, num_outputs),\n",
    "                   nn.Identity())\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.0)\n",
    "\n",
    "# initialize my network using torch W, b for each layer\n",
    "W0 = net[0].weight.detach().numpy().T\n",
    "b0 = net[0].bias.detach().numpy().T\n",
    "W1 = net[2].weight.detach().numpy().T\n",
    "b1 = net[2].bias.detach().numpy().T\n",
    "\n",
    "mlp1 = MLP1(num_inputs, num_outputs, num_hiddens)\n",
    "mlp1.layers[0].W = W0\n",
    "mlp1.layers[0].b = b0\n",
    "mlp1.layers[1].W = W1\n",
    "mlp1.layers[1].b = b1\n",
    "\n",
    "my_out = mlp1.forward(X)\n",
    "torch_out = net(Xt)\n",
    "\n",
    "print('MyTorch:\\n', my_out, '\\n')\n",
    "print('PyTorch:\\n', torch_out.data, '\\n')\n",
    "print('Difference:', np.linalg.norm(my_out - torch_out.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f2322",
   "metadata": {},
   "source": [
    "## Test `backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1be0a6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in dLdW0: 5.073911425469867e-08\n",
      "Difference in dLdb0: 4.622361428754544e-09\n",
      "Difference in dLdW1: 3.821583479869518e-08\n",
      "Difference in dLdb1: 2.3672265170171044e-08\n"
     ]
    }
   ],
   "source": [
    "my_mse_fn = mynn.MSELoss()\n",
    "my_mse = my_mse_fn.forward(my_out, Y)\n",
    "dLdZ = my_mse_fn.backward()\n",
    "mlp1.backward(dLdZ)\n",
    "my_dLdW0 = mlp1.layers[0].dLdW.T\n",
    "my_dLdb0 = mlp1.layers[0].dLdb\n",
    "my_dLdW1 = mlp1.layers[1].dLdW.T\n",
    "my_dLdb1 = mlp1.layers[1].dLdb\n",
    "\n",
    "optimizer.zero_grad()\n",
    "torch_loss_fn = nn.MSELoss()\n",
    "torch_loss = torch_loss_fn(torch_out, Yt)\n",
    "torch_loss.backward(retain_graph=True)\n",
    "torch_dLdW0 = net[0].weight.grad.data\n",
    "torch_dLdb0 = net[0].bias.grad.data\n",
    "torch_dLdW1 = net[2].weight.grad.data\n",
    "torch_dLdb1 = net[2].bias.grad.data\n",
    "\n",
    "print('Difference in dLdW0:', np.linalg.norm(my_dLdW0 - torch_dLdW0.data.numpy()))\n",
    "print('Difference in dLdb0:', np.linalg.norm(my_dLdb0.flatten() - torch_dLdb0.data.numpy()))\n",
    "print('Difference in dLdW1:', np.linalg.norm(my_dLdW1 - torch_dLdW1.data.numpy()))\n",
    "print('Difference in dLdb1:', np.linalg.norm(my_dLdb1.flatten() - torch_dLdb1.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e93550",
   "metadata": {},
   "source": [
    "## Test a single optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4743916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in Wk0: 0.4484992422899716\n",
      "Difference in bk0: 0.11698040828986507\n",
      "Difference in Wk1: 0.0828025291773284\n",
      "Difference in bk1: 0.7600495461767015\n"
     ]
    }
   ],
   "source": [
    "# my SGD step\n",
    "my_optimizer = mytorch.optim.SGD(mlp1, lr=1)\n",
    "my_optimizer.step()\n",
    "my_Wk0 = mlp1.layers[0].W\n",
    "my_bk0 = mlp1.layers[0].b\n",
    "my_Wk1 = mlp1.layers[1].W\n",
    "my_bk1 = mlp1.layers[1].b\n",
    "\n",
    "# torch SGD step\n",
    "optimizer.zero_grad()\n",
    "torch_loss.backward(retain_graph=True)\n",
    "optimizer.step()\n",
    "torch_Wk0 = net[0].weight.data\n",
    "torch_bk0 = net[0].bias.data\n",
    "torch_Wk1 = net[2].weight.data\n",
    "torch_bk1 = net[2].bias.data\n",
    "\n",
    "print('Difference in Wk0:', np.linalg.norm(my_Wk0 - torch_Wk0.numpy().T))\n",
    "print('Difference in bk0:', np.linalg.norm(my_bk0.flatten() - torch_bk0.numpy()))\n",
    "print('Difference in Wk1:', np.linalg.norm(my_Wk1 - torch_Wk1.numpy().T))\n",
    "print('Difference in bk1:', np.linalg.norm(my_bk1.flatten() - torch_bk1.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce88760eaf01839c4fda2e72a94ec028ac275858c03c4e18a870db35bb526096"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
