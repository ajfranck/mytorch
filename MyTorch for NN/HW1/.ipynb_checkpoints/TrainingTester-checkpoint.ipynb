{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52132158",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "import mytorch\n",
    "from mytorch import nn as mynn\n",
    "from models import MLP0, MLP1, MLP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed41ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure trainer for use with MyTorch\n",
    "class Trainer(d2l.Trainer):           \n",
    "    def fit_epoch(self):\n",
    "        self.model.train()\n",
    "        for batch in self.train_dataloader:\n",
    "            loss = self.model.training_step(self.prepare_batch(batch))\n",
    "            self.optim.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                dLdZ = self.model.loss_fn.backward()\n",
    "                self.model.backward(dLdZ)\n",
    "                self.optim.step()\n",
    "            self.train_batch_idx += 1\n",
    "        if self.val_dataloader is None:\n",
    "            return\n",
    "        self.model.eval()\n",
    "        for batch in self.val_dataloader:\n",
    "            with torch.no_grad():\n",
    "                self.model.validation_step(self.prepare_batch(batch))\n",
    "            self.val_batch_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08311b",
   "metadata": {},
   "source": [
    "# Regression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56adb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_outputs = 1\n",
    "num_train = 1000\n",
    "num_val = 500\n",
    "\n",
    "w = np.array([2, -3.4]).astype('float')\n",
    "b = np.array(4.2).astype('float')\n",
    "\n",
    "data = d2l.SyntheticRegressionData(w=torch.tensor(w).float(), b=b, num_train=num_train, num_val=num_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e0c50",
   "metadata": {},
   "source": [
    "### PyTorch Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80777e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit d2l/torch model\n",
    "class SimpleLinear(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Linear(num_inputs, num_outputs)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        loss_fn = nn.MSELoss()\n",
    "        return loss_fn(Y_hat, Y)\n",
    "    \n",
    "net = SimpleLinear(num_inputs, num_outputs, lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)\n",
    "\n",
    "print(net.net.weight.data)\n",
    "print(net.net.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1448a2b",
   "metadata": {},
   "source": [
    "### MyTorch Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10214e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put our MyTorch network into d2l format\n",
    "class MyLinear(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = mynn.Linear(num_inputs, num_outputs)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net.forward(X.numpy())\n",
    "    \n",
    "    def backward(self, dLdZ):\n",
    "        self.net.backward(dLdZ)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        loss_fn = mynn.MSELoss()\n",
    "        Y = Y.detach().numpy()  \n",
    "        loss = loss_fn.forward(Y_hat, Y)\n",
    "        self.loss_fn = loss_fn\n",
    "        return torch.tensor(loss)\n",
    "    \n",
    "    def loss_fun(self):\n",
    "        return self.loss_fn\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optim = mytorch.optim.SGD(self.net, lr=self.lr)\n",
    "        return(optim)\n",
    "    \n",
    "net = MyLinear(num_inputs, num_outputs, lr=0.01)\n",
    "trainer = Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)\n",
    "\n",
    "print(net.net.W)\n",
    "print(net.net.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056b27e7",
   "metadata": {},
   "source": [
    "### PyTorch MLP0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072870d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit d2l/torch model\n",
    "class SimpleMLP0(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Linear(num_inputs, num_outputs))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        loss_fn = nn.MSELoss()\n",
    "        return loss_fn(Y_hat, Y)\n",
    "    \n",
    "net = SimpleMLP0(num_inputs, num_outputs, lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)\n",
    "\n",
    "print(net.net[0].weight.data)\n",
    "print(net.net[0].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47fedaa",
   "metadata": {},
   "source": [
    "### MyTorch MLP0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b65c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put our MyTorch network into d2l format\n",
    "class MyMLP0(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = MLP0(num_inputs, num_outputs)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net.forward(X.numpy())\n",
    "    \n",
    "    def backward(self, dLdZ):\n",
    "        self.net.backward(dLdZ)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        loss_fn = mynn.MSELoss()\n",
    "        Y = Y.detach().numpy()  \n",
    "        loss = loss_fn.forward(Y_hat, Y)\n",
    "        self.loss_fn = loss_fn\n",
    "        return torch.tensor(loss)\n",
    "    \n",
    "    def loss_fun(self):\n",
    "        return self.loss_fn\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optim = mytorch.optim.SGD(self.net, lr=self.lr)\n",
    "        return(optim)\n",
    "    \n",
    "net = MyMLP0(num_inputs, num_outputs, lr=0.01)\n",
    "trainer = Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)\n",
    "\n",
    "print(net.net.layers[0].W)\n",
    "print(net.net.layers[0].b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c7d076",
   "metadata": {},
   "source": [
    "### PyTorch MLP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52af1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit d2l/torch model\n",
    "class SimpleMLP1(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(num_hiddens, num_outputs))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        loss_fn = nn.MSELoss()\n",
    "        return loss_fn(Y_hat, Y)\n",
    "    \n",
    "num_hiddens = 10\n",
    "net = SimpleMLP1(num_inputs, num_outputs, num_hiddens, lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd304f97",
   "metadata": {},
   "source": [
    "### MyTorch MLP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c958ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put our MyTorch network into d2l format\n",
    "class MyMLP1(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = MLP1(num_inputs, num_outputs, num_hiddens)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net.forward(X.numpy())\n",
    "    \n",
    "    def backward(self, dLdZ):\n",
    "        self.net.backward(dLdZ)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        loss_fn = mynn.MSELoss()\n",
    "        Y = Y.detach().numpy()  \n",
    "        loss = loss_fn.forward(Y_hat, Y)\n",
    "        self.loss_fn = loss_fn\n",
    "        return torch.tensor(loss)\n",
    "    \n",
    "    def loss_fun(self):\n",
    "        return self.loss_fn\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optim = mytorch.optim.SGD(self.net, lr=self.lr)\n",
    "        return(optim)\n",
    "    \n",
    "num_hiddens = 10\n",
    "net = MyMLP1(num_inputs, num_outputs, num_hiddens, lr=0.01)\n",
    "trainer = Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e17a7",
   "metadata": {},
   "source": [
    "### PyTorch MLP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit d2l/torch model\n",
    "class SimpleMLP4(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Flatten(),\n",
    "                                 nn.Linear(num_inputs, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_hiddens, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_hiddens, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_hiddens, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_hiddens, num_outputs))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        loss_fn = nn.MSELoss()\n",
    "        return loss_fn(Y_hat, Y)\n",
    "    \n",
    "num_hiddens = 10\n",
    "net = SimpleMLP4(num_inputs, num_outputs, num_hiddens, lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682c628",
   "metadata": {},
   "source": [
    "### MyTorch MLP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put our MyTorch network into d2l format\n",
    "class MyMLP4(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = MLP4(num_inputs, num_outputs, num_hiddens)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net.forward(X.numpy())\n",
    "    \n",
    "    def backward(self, dLdZ):\n",
    "        self.net.backward(dLdZ)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        loss_fn = mynn.MSELoss()\n",
    "        Y = Y.detach().numpy()  \n",
    "        loss = loss_fn.forward(Y_hat, Y)\n",
    "        self.loss_fn = loss_fn\n",
    "        return torch.tensor(loss)\n",
    "    \n",
    "    def loss_fun(self):\n",
    "        return self.loss_fn\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optim = mytorch.optim.SGD(self.net, lr=self.lr)\n",
    "        return(optim)\n",
    "    \n",
    "num_hiddens = 10\n",
    "net = MyMLP4(num_inputs, num_outputs, num_hiddens, lr=0.01)\n",
    "trainer = Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b09245d",
   "metadata": {},
   "source": [
    "# Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ee202",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.FashionMNIST(batch_size=256)\n",
    "num_inputs = 784\n",
    "num_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40916fb5",
   "metadata": {},
   "source": [
    "### PyTorch Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit d2l/torch model\n",
    "class SimpleLinear(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Flatten(),\n",
    "                                 nn.Linear(num_inputs, num_outputs))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        return loss_fn(Y_hat, Y)\n",
    "    \n",
    "net = SimpleLinear(num_inputs, num_outputs, lr=0.1)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b92237",
   "metadata": {},
   "source": [
    "### MyTorch Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea111ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put our MyTorch network into d2l format\n",
    "class MyLinear(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = mynn.Linear(num_inputs, num_outputs)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        m = nn.Flatten()\n",
    "        X = m(X)\n",
    "        X = X.numpy()\n",
    "        return self.net.forward(X)\n",
    "    \n",
    "    def backward(self, dLdZ):\n",
    "        self.net.backward(dLdZ)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        Y = nn.functional.one_hot(Y)\n",
    "        Y = Y.detach().numpy()\n",
    "        loss_fn = mynn.CrossEntropyLoss()\n",
    "        self.loss_fn = loss_fn\n",
    "        loss = loss_fn.forward(Y_hat, Y)\n",
    "        return torch.tensor(loss)\n",
    "    \n",
    "    def loss_fun(self):\n",
    "        return self.loss_fn\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optim = mytorch.optim.SGD(self.net, lr=self.lr)\n",
    "        return(optim)\n",
    "    \n",
    "net = MyLinear(num_inputs, num_outputs, lr=0.1)\n",
    "trainer = Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4208f1",
   "metadata": {},
   "source": [
    "### PyTorch MLP0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit d2l/torch model\n",
    "class SimpleMLP0(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Flatten(),\n",
    "                                 nn.Linear(num_inputs, num_outputs))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        return loss_fn(Y_hat, Y)\n",
    "    \n",
    "net = SimpleMLP0(num_inputs, num_outputs, lr=0.1)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c6ba5",
   "metadata": {},
   "source": [
    "### MyTorch MLP0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put our MyTorch network into d2l format\n",
    "class MyMLP0(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = MLP0(num_inputs, num_outputs)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        m = nn.Flatten()\n",
    "        X = m(X)\n",
    "        return self.net.forward(X.numpy())\n",
    "    \n",
    "    def backward(self, dLdZ):\n",
    "        self.net.backward(dLdZ)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        Y = nn.functional.one_hot(Y)\n",
    "        Y = Y.detach().numpy()  \n",
    "        loss_fn = mynn.CrossEntropyLoss()\n",
    "        loss = loss_fn.forward(Y_hat, Y)\n",
    "        self.loss_fn = loss_fn\n",
    "        return torch.tensor(loss)\n",
    "    \n",
    "    def loss_fun(self):\n",
    "        return self.loss_fn\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optim = mytorch.optim.SGD(self.net, lr=self.lr)\n",
    "        return(optim)\n",
    "    \n",
    "net = MyMLP0(num_inputs, num_outputs, lr=0.1)\n",
    "trainer = Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b81ba",
   "metadata": {},
   "source": [
    "### PyTorch MLP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cabae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit d2l/torch model\n",
    "class SimpleMLP1(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Flatten(),\n",
    "                                 nn.Linear(num_inputs, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_hiddens, num_outputs))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        return loss_fn(Y_hat, Y)\n",
    "    \n",
    "num_hiddens = 20\n",
    "net = SimpleMLP1(num_inputs, num_outputs, num_hiddens, lr=0.1)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ded908",
   "metadata": {},
   "source": [
    "### MyTorch MLP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd829b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put our MyTorch network into d2l format\n",
    "class MyMLP1(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = MLP1(num_inputs, num_outputs, num_hiddens)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        m = nn.Flatten()\n",
    "        X = m(X)\n",
    "        return self.net.forward(X.numpy())\n",
    "    \n",
    "    def backward(self, dLdZ):\n",
    "        self.net.backward(dLdZ)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        Y = nn.functional.one_hot(Y)\n",
    "        Y = Y.detach().numpy()  \n",
    "        loss_fn = mynn.CrossEntropyLoss()\n",
    "        loss = loss_fn.forward(Y_hat, Y)\n",
    "        self.loss_fn = loss_fn\n",
    "        return torch.tensor(loss)\n",
    "    \n",
    "    def loss_fun(self):\n",
    "        return self.loss_fn\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optim = mytorch.optim.SGD(self.net, lr=self.lr)\n",
    "        return(optim)\n",
    "    \n",
    "num_hiddens = 20\n",
    "net = MyMLP1(num_inputs, num_outputs, num_hiddens, lr=0.1)\n",
    "trainer = Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add96f61",
   "metadata": {},
   "source": [
    "### PyTorch MLP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c54245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit d2l/torch model\n",
    "class SimpleMLP4(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Flatten(),\n",
    "                                 nn.Linear(num_inputs, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_hiddens, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_hiddens, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_hiddens, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_hiddens, num_outputs))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        return loss_fn(Y_hat, Y)\n",
    "    \n",
    "num_hiddens = 20\n",
    "net = SimpleMLP4(num_inputs, num_outputs, num_hiddens, lr=0.1)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e545c836",
   "metadata": {},
   "source": [
    "### MyTorch MLP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75745694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put our MyTorch network into d2l format\n",
    "class MyMLP4(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = MLP4(num_inputs, num_outputs, num_hiddens)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        m = nn.Flatten()\n",
    "        X = m(X)\n",
    "        return self.net.forward(X.numpy())\n",
    "    \n",
    "    def backward(self, dLdZ):\n",
    "        self.net.backward(dLdZ)\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        Y = nn.functional.one_hot(Y)\n",
    "        Y = Y.detach().numpy()  \n",
    "        loss_fn = mynn.CrossEntropyLoss()\n",
    "        loss = loss_fn.forward(Y_hat, Y)\n",
    "        self.loss_fn = loss_fn\n",
    "        return torch.tensor(loss)\n",
    "    \n",
    "    def loss_fun(self):\n",
    "        return self.loss_fn\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optim = mytorch.optim.SGD(self.net, lr=self.lr)\n",
    "        return(optim)\n",
    "    \n",
    "num_hiddens = 20\n",
    "net = MyMLP4(num_inputs, num_outputs, num_hiddens, lr=0.1)\n",
    "trainer = Trainer(max_epochs=10)\n",
    "trainer.fit(net, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3d46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
