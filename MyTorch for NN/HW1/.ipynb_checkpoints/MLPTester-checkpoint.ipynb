{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe5a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "import mytorch\n",
    "from mytorch import nn as mynn\n",
    "from models import MLP0, MLP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c637116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up synthetic data\n",
    "N = 10\n",
    "num_inputs = 7\n",
    "num_outputs = 2\n",
    "\n",
    "# numpy/our versions\n",
    "W = np.random.rand(num_inputs, num_outputs)\n",
    "b = np.random.rand(num_outputs)\n",
    "X = np.random.randn(N, num_inputs)\n",
    "Y = X @ W + np.outer(np.ones(N), b) + 0.5 * np.random.randn(N, num_outputs)\n",
    "\n",
    "# converted torch versions\n",
    "Xt = torch.tensor(X).float()\n",
    "Wt = torch.tensor(W).float()\n",
    "bt = torch.tensor(b).float()\n",
    "Yt = torch.tensor(Y).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2dca35",
   "metadata": {},
   "source": [
    "# MLP0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228bf728",
   "metadata": {},
   "source": [
    "## Test `forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model and fix weights to true values\n",
    "mlp0 = MLP0(num_inputs, num_outputs)\n",
    "mlp0.layers[0].W = W\n",
    "mlp0.layers[0].b = b\n",
    "\n",
    "# initialize torch model, loss, optimizer\n",
    "net = nn.Sequential(nn.Linear(num_inputs, num_outputs))\n",
    "net[0].weight = nn.Parameter(Wt.T)\n",
    "net[0].bias = nn.Parameter(bt)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1, momentum=0.0)\n",
    "\n",
    "my_out = mlp0.forward(X)\n",
    "torch_out = net(Xt)\n",
    "\n",
    "print('MyTorch:\\n', my_out, '\\n')\n",
    "print('PyTorch:\\n', torch_out.data, '\\n')\n",
    "print('Difference:', np.linalg.norm(my_out - torch_out.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700cdb6",
   "metadata": {},
   "source": [
    "## Test `backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f75882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mse_fn = mynn.MSELoss()\n",
    "my_mse = my_mse_fn.forward(my_out, Y)\n",
    "dLdZ = my_mse_fn.backward()\n",
    "mlp0.backward(dLdZ)\n",
    "my_dLdW = mlp0.layers[0].dLdW\n",
    "my_dLdb = mlp0.layers[0].dLdb\n",
    "\n",
    "optimizer.zero_grad()\n",
    "torch_loss_fn = nn.MSELoss()\n",
    "torch_loss = torch_loss_fn(torch_out, Yt)\n",
    "torch_loss.backward(retain_graph=True)\n",
    "torch_dLdW = net[0].weight.grad.data\n",
    "torch_dLdb = net[0].bias.grad.data\n",
    "\n",
    "print('MyTorch dLdW:\\n', my_dLdW, '\\n')\n",
    "print('PyTorch dLdW:\\n', torch_dLdW.T, '\\n')\n",
    "print('MyTorch dLdb:\\n', my_dLdb, '\\n')\n",
    "print('PyTorch dLdb:\\n', torch_dLdb, '\\n')\n",
    "\n",
    "print('Difference in dLdW:', np.linalg.norm(my_dLdW.T - torch_dLdW.numpy()))\n",
    "print('Difference in dLdb:', np.linalg.norm(my_dLdb.flatten() - torch_dLdb.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9df06",
   "metadata": {},
   "source": [
    "## Test a single optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aab683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my SGD step\n",
    "my_optimizer = mytorch.optim.SGD(mlp0, lr=1)\n",
    "my_optimizer.step()\n",
    "my_Wk = mlp0.layers[0].W\n",
    "my_bk = mlp0.layers[0].b\n",
    "\n",
    "# torch SGD step\n",
    "optimizer.zero_grad()\n",
    "torch_loss.backward(retain_graph=True)\n",
    "optimizer.step()\n",
    "torch_Wk = net[0].weight.data\n",
    "torch_bk = net[0].bias.data\n",
    "\n",
    "print('MyTorch Wk:\\n', my_Wk, '\\n')\n",
    "print('PyTorch Wk:\\n', torch_Wk.T, '\\n')\n",
    "print('MyTorch bk:\\n', my_bk, '\\n')\n",
    "print('PyTorch bk:\\n', torch_bk)\n",
    "\n",
    "print('Difference in Wk:', np.linalg.norm(my_Wk.T - torch_Wk.numpy()))\n",
    "print('Difference in bk:', np.linalg.norm(my_bk.flatten() - torch_bk.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babef7d0",
   "metadata": {},
   "source": [
    "# MLP1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f72747c",
   "metadata": {},
   "source": [
    "## Test `forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens=3\n",
    "\n",
    "# initialize torch model, loss, optimizer\n",
    "net = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(num_hiddens, num_outputs),\n",
    "                   nn.ReLU())\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.0)\n",
    "\n",
    "# initialize my network using torch W, b for each layer\n",
    "W0 = net[0].weight.detach().numpy().T\n",
    "b0 = net[0].bias.detach().numpy().T\n",
    "W1 = net[2].weight.detach().numpy().T\n",
    "b1 = net[2].bias.detach().numpy().T\n",
    "\n",
    "mlp1 = MLP1(num_inputs, num_outputs, num_hiddens)\n",
    "mlp1.layers[0].W = W0\n",
    "mlp1.layers[0].b = b0\n",
    "mlp1.layers[1].W = W1\n",
    "mlp1.layers[1].b = b1\n",
    "\n",
    "my_out = mlp1.forward(X)\n",
    "torch_out = net(Xt)\n",
    "\n",
    "print('MyTorch:\\n', my_out, '\\n')\n",
    "print('PyTorch:\\n', torch_out.data, '\\n')\n",
    "print('Difference:', np.linalg.norm(my_out - torch_out.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f2322",
   "metadata": {},
   "source": [
    "## Test `backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be0a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mse_fn = mynn.MSELoss()\n",
    "my_mse = my_mse_fn.forward(my_out, Y)\n",
    "dLdZ = my_mse_fn.backward()\n",
    "mlp1.backward(dLdZ)\n",
    "my_dLdW0 = mlp1.layers[0].dLdW\n",
    "my_dLdb0 = mlp1.layers[0].dLdb\n",
    "my_dLdW1 = mlp1.layers[1].dLdW\n",
    "my_dLdb1 = mlp1.layers[1].dLdb\n",
    "\n",
    "optimizer.zero_grad()\n",
    "torch_loss_fn = nn.MSELoss()\n",
    "torch_loss = torch_loss_fn(torch_out, Yt)\n",
    "torch_loss.backward(retain_graph=True)\n",
    "torch_dLdW0 = net[0].weight.grad.data\n",
    "torch_dLdb0 = net[0].bias.grad.data\n",
    "torch_dLdW1 = net[2].weight.grad.data\n",
    "torch_dLdb1 = net[2].bias.grad.data\n",
    "\n",
    "print('Difference in dLdW0:', np.linalg.norm(my_dLdW0 - torch_dLdW0.data.numpy()))\n",
    "print('Difference in dLdb0:', np.linalg.norm(my_dLdb0.flatten() - torch_dLdb0.data.numpy()))\n",
    "print('Difference in dLdW1:', np.linalg.norm(my_dLdW1 - torch_dLdW1.data.numpy()))\n",
    "print('Difference in dLdb1:', np.linalg.norm(my_dLdb1.flatten() - torch_dLdb1.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e93550",
   "metadata": {},
   "source": [
    "## Test a single optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4743916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my SGD step\n",
    "my_optimizer = mytorch.optim.SGD(mlp1, lr=1)\n",
    "my_optimizer.step()\n",
    "my_Wk0 = mlp1.layers[0].W\n",
    "my_bk0 = mlp1.layers[0].b\n",
    "my_Wk1 = mlp1.layers[1].W\n",
    "my_bk1 = mlp1.layers[1].b\n",
    "\n",
    "# torch SGD step\n",
    "optimizer.zero_grad()\n",
    "torch_loss.backward(retain_graph=True)\n",
    "optimizer.step()\n",
    "torch_Wk0 = net[0].weight.data\n",
    "torch_bk0 = net[0].bias.data\n",
    "torch_Wk1 = net[2].weight.data\n",
    "torch_bk1 = net[2].bias.data\n",
    "\n",
    "print('Difference in Wk0:', np.linalg.norm(my_Wk0 - torch_Wk0.numpy().T))\n",
    "print('Difference in bk0:', np.linalg.norm(my_bk0.flatten() - torch_bk0.numpy()))\n",
    "print('Difference in Wk1:', np.linalg.norm(my_Wk1 - torch_Wk1.numpy().T))\n",
    "print('Difference in bk1:', np.linalg.norm(my_bk1.flatten() - torch_bk1.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
